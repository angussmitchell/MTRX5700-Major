\subsection{Sound Processing}

%%ANGUS: for you to discuss how we achieved what's been specified in the intro - make subsubsections for like brief description of how we're using aubio, beat detection, clustering, etc...

%% fix the intro/background if you feel inaccurately describes the sound processing bits (doesn't need to be too detailed in the intro/background) - your time to shine is here and in section 4

The objective of the sound processing module was to extract information from the song, in both live, and non live cases. Non live processing involves parsing the entire song before it is played, while the live case involves playing and processing the song 'on-the-fly'. Processing in the non live case is significantly easier, as sufficient time is available for calculations, and context of each section in the song can be more easily determined. This module extracted the tempo of the song and attempted to classify how the song's sections change throughout. The classification task ultimately decides what type of dance moves or "feel" is appropriate in different sections of the song, while the tempo informs the accurate timing of dance moves.

\subsection{Tempo Detection}
Tempo detection utilised Aubio's tempo and onset detection library. The python library consists of a set of python wrapper classes which can be used to call the C implemented Aubio library. \\
\\
TALK ABOUT SWITCHING BETWEEN ONSET AND TEMPO DETECTION DURING LIVE\\
\\
TALK ABOUT RELIABILITY IN NON LIVE CASE

\subsection{Section Classification}
The goal of classification was to divide the song into several distinct sections which can intuitively be called ``intro", ``verse", ``chorus" or ``instrumental". Automating this process posed a significant challenge. The approach that was taken was a machine learning approach, using a set of features in the song to classify the type of each section of the song. \\
\\
The first step in this process was to extract some meaningful features from the song at each time step. It was determined that the best way to achieve this was to use MFCC (mel-frequency cepstral coefficients). This is a method of feature extraction often used in genre classification and speech recognition \cite{rajani2009supervised} \cite{haggblade2011music}. This technique yields a set of meaningful features that are much less expensive to process than the raw sound samples. Use of a down sampled version of the song as a feature vector was considered, however, doing this would result in the loss of high frequency components and would still be computationally expensive. The \textit{scikit.talkbo} python library was used to extract the MFCCs. This module allowed a maximum of 40 MFCCs for each song ``chunk" that was processed. This maximum value was chosen, in order to yield a maximally detailed feature vector to the following step.\\
\\
The next step in this process involved reducing the set of 40 dimensional feature set down to a lower dimension for processing and visualisation or clusters. PCA was applied, a process which reduces high dimensional features down by projecting less deterministic (or less independent) features onto the axis of the more independent features. The result was a set of two features, which could be plotted against the third features, which was time. \\
\\
After this, a simple means shift clustering algorithm was applied to separate out song ``sections". The Algorithm was applied to the three dimensional (two frequency dimensions plus one time dimension) features\ vector to find distinct sections in the song. It should be noted that this algorithm is merely "clustering" the song chunks according to their frequency characteristics and their time in the song - it is not performing classification. Sections in the song tend to share distinct frequency components, and are also grouped together in time (i.e. if the current section is a sparse instrumental section, it is likely that the sections immediately after and before are also sparse instrumental sections). Mean shift is a clustering algorithm very similar to the popular k-means algorithm. The main difference is that mean shift uses cluster density to determine the number of clusters automatically. For this project the ``sklearn.cluster" implementation was used. This required the input of a bandwidth and a minimum samples cut-off. The bandwidth determines the kernel size (which is used to determine how dense a cluster needs to be, to be recognised), while the minimum samples input determines the minimum number of samples needed to justify a new cluster.\\
\\
The final step was to compare the extracted cluster centroids to known cluster centroids of different song sections such as ``sparse", ``verse", ``rock chorus" and ``EDM (electronic dance music)chorus". The known cluster centroids were created by averaging a large number of MFCCs taken from a variety of songs. In doing this, we found the typical MFCC values for the required sections (``sparse", ``verse" etc). This step allowed the clusters to be classified in terms of their similarity to the sample song sections. Ideally, a multilayer perceptron would be trained on these MFCC features, and used to classify sections based on the learned knowledge. Due to resource and time constraints however, cluster centroids were simply labeled by finding the closest class (in terms of euclidean distance in the features space).