\subsection{Sound Processing}

%%ANGUS: for you to discuss how we achieved what's been specified in the intro - make subsubsections for like brief description of how we're using aubio, beat detection, clustering, etc...

%% fix the intro/background if you feel inaccurately describes the sound processing bits (doesn't need to be too detailed in the intro/background) - your time to shine is here and in section 4

The objective of the sound processing module was to extract information from the song in both the live and non-live cases. Non-live processing involves parsing the entire song before it is played, while the live case involves playing and processing the song on-the-fly. Processing in the non-live case is significantly easier, as sufficient time is available for calculations, and context of each section in the song can be more easily determined. This module extracts the tempo of the song and attempts to classify how mood and sparseness change throughout the song. The classification task ultimately decides what type of dance moves or ``feel" is appropriate in different sections of the song, while the tempo informs the accurate timing of dance moves.

\subsection{Tempo Detection}
Tempo detection utilises Aubio's tempo and onset detection library. The python library consists of a set of python wrapper classes which can be used to call the C-implemented Aubio library. \\
\\
Aubio returns a confidence level as a floating point decimal number. When this confidence level falls below the
threshold set in the drone dancer module, the beat onset event is used for dancing rather than the beat event.
Aubio continues to fire the beat event with its last known BPM, so in songs where BPM changes this is an issue.
\\
A limitation of Aubio's beat detection is that it does not return the BPM of the current sample, but for samples
in the past. For audio that is pre-processed, this is not an issue as all the beat times can be offset by a given
amount, however in the case of live audio this is an issue. In tests conducted with live audio, reaction to a
change in BPM occurred approximately 8 beats after the bpm had changed. Our solution to this issue was to delay
the outgoing audio in the case of live line-in input in order to allow the beat times to be offset to their
correct locations. However, audio in from a microphone is unable to be delayed, which is why the drone dancer
module begins using the beat onset event rather than the beat event when Aubio's bpm confidence level drops below
a threshold value.

\subsection{Section Classification}
The goal of classification was to divide the song into several distinct sections which can intuitively be called ``intro", ``verse", ``chorus" or ``instrumental". Automating this process posed a significant challenge. The approach that was taken was a machine learning approach, using a set of features in the song to classify the type of each section of the song. \\
\\
The first step in this process was to extract meaningful features from the song at each time step. It was determined that the best way to achieve this was to use MFCCs (mel-frequency cepstral coefficients.) This is a method of feature extraction often used in genre classification and speech recognition (1)(2). This technique yields a set of meaningful features that are much less expensive to process than the raw sound samples. Use of a down sampled version of the song as a feature vector was considered, however, doing this would result in the loss of high frequency components and would still be computationally expensive. The ``scikit.talkbox" python library was used to extract the MFCCs. This module allowed a maximum of 40 MFCCs for each song ``chunk" that was processed. This maximum value was chosen, in order to yield a maximally detailed feature vector to the following step.\\
\\
The next step in this process involved reducing the set of 40 dimensional feature set down to a lower dimension for processing and visualisation or clusters. PCA was applied, a process which reduces high dimensional features down by projecting less deterministic (or less independent) features onto the axis of the more independent features. The result was a set of two features, which could be plotted against the third features, which was time. \\
\\
After this, a simple means shift clustering algorithm was applied to separate out song ``sections". The Algorithm was applied to the three dimensional (two frequency dimensions plus one time dimension) features\ vector to find distinct sections in the song. It should be noted that this algorithm is merely "clustering" the song chunks according to their frequency characteristics and their time in the song - it is not performing classification. Sections in the song tend to share distinct frequency components, and are also grouped together in time (i.e. if the current section is a sparse instrumental section, it is likely that the sections immediately after and before are also sparse instrumental sections.) Mean shift is a clustering algorthm very similar to the popular k-means algorythm. The main difference is that mean shift uses cluster density to determine the number of clusters automatically. For this project the ``sklearn.cluster" implementation was used. This required the input of a bandwidth and a minimum samples cuttoff. The bandwidth determines the kernel size (which is used to determine how dense a cluster needs to be, to be recognised ), while the minimum samples input determines the minimum number of samples needed to justify a new cluster.\\
\\
The final step was to compare the extracted cluster centroids to known cluster centroids of different song sections such as ``sparse", ``verse", ``rock chorus" and "EDM (electronic dance music)chorus". The known cluster centroids were created by averaging a large number of MFCCs taken from a variety of songs. In doing this, we found the typical MFCC values for the required sections (``sparse", ``verse" etc). This step allowed the clusters to be classified in terms of their similarity to the sample song sections. Ideally, a multilayer perceptron would be trained on these MFCC features, and used to classify sections based on thet learned knowledge. Due to resource and time constraints however, cluster centroids were simply labeled by finding the closest class (in terms of euclidean distance in the features space).



(1) http://cs229.stanford.edu/proj2009/RajaniEkkizogloy.pdf
(2)	http://cs229.stanford.edu/proj2011/HaggbladeHongKao-MusicGenreClassification.pdf
